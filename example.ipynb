{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import OGB_MAG\n",
    "\n",
    "dataset = OGB_MAG(root='./data', preprocess='metapath2vec')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mpaper\u001b[0m={\n",
       "    x=[736389, 128],\n",
       "    year=[736389],\n",
       "    y=[736389],\n",
       "    train_mask=[736389],\n",
       "    val_mask=[736389],\n",
       "    test_mask=[736389]\n",
       "  },\n",
       "  \u001b[1mauthor\u001b[0m={ x=[1134649, 128] },\n",
       "  \u001b[1minstitution\u001b[0m={ x=[8740, 128] },\n",
       "  \u001b[1mfield_of_study\u001b[0m={ x=[59965, 128] },\n",
       "  \u001b[1m(author, affiliated_with, institution)\u001b[0m={ edge_index=[2, 1043998] },\n",
       "  \u001b[1m(author, writes, paper)\u001b[0m={ edge_index=[2, 7145660] },\n",
       "  \u001b[1m(paper, cites, paper)\u001b[0m={ edge_index=[2, 5416271] },\n",
       "  \u001b[1m(paper, has_topic, field_of_study)\u001b[0m={ edge_index=[2, 7505078] }\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "def nx2hetero(G):\n",
    "\tids_by_type = {\n",
    "\t\t\"playlist\": {},\n",
    "\t\t\"track\": {},\n",
    "\t\t\"artist\": {},\n",
    "\t\t\"album\": {}\n",
    "\t}\n",
    "\tdef node_id(type, id, exception=False):\n",
    "\t\td = ids_by_type[type]\n",
    "\t\tif id not in d:\n",
    "\t\t\tif exception:\n",
    "\t\t\t\traise Exception(f'node {id} not found for type {type}')\n",
    "\t\t\td[id] = len(d)\n",
    "\t\treturn d[id]\n",
    "\n",
    "\t\n",
    "\tnode_features_by_type = {\n",
    "\t\t\"playlist\": [],\n",
    "\t\t\"track\": [],\n",
    "\t\t\"artist\": [],\n",
    "\t\t\"album\": []\n",
    "\t}\n",
    "\tfor node in G.nodes(data=True):\n",
    "\t\tt = node[1][\"node_type\"]\n",
    "\t\tif t == \"playlist\":\n",
    "\t\t\tnode_id(\"playlist\", node[0])\n",
    "\t\t\tnode_features_by_type[\"playlist\"] += [[node[1][\"num_followers\"], 1, 1, 1, 1, 1]]\n",
    "\t\telif t == \"track\":\n",
    "\t\t\tnode_id(\"track\", node[0])\n",
    "\t\t\tnode_features_by_type[\"track\"] += [[node[1][\"duration\"], 1, 1, 1, 1, 1]]\n",
    "\t\telif t == \"artist\":\n",
    "\t\t\tnode_id(\"artist\", node[0])\n",
    "\t\t\tnode_features_by_type[\"artist\"] += [[1, 1, 1, 1, 1, 1]]\n",
    "\t\telif t == \"album\":\n",
    "\t\t\tnode_id(\"album\", node[0])\n",
    "\t\t\tnode_features_by_type[\"album\"] += [[1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "\n",
    "\tedge_index_by_type = {\n",
    "\t\t(\"playlist\", \"contains\", \"track\"): [],\n",
    "\t\t(\"album\", \"includes\", \"track\"): [],\n",
    "\t\t(\"artist\", \"authors\", \"track\"): []\n",
    "\t}\n",
    "\tfor edge in G.edges(data=True):\n",
    "\t\tif G[edge[0]][edge[1]][\"edge_type\"] == \"track-playlist\":\n",
    "\t\t\ts_id = node_id(\"track\", edge[0], exception=True)\n",
    "\t\t\tt_id = node_id(\"playlist\", edge[1], exception=True)\n",
    "\t\t\t\n",
    "\t\t\tedge_index_by_type[(\"playlist\", \"contains\", \"track\")] += [(t_id, s_id)]\n",
    "\t\telif G[edge[0]][edge[1]][\"edge_type\"] == \"track-album\":\n",
    "\t\t\ts_id = node_id(\"track\", edge[0], exception=True)\n",
    "\t\t\tt_id = node_id(\"album\", edge[1], exception=True)\n",
    "\t\t\t\n",
    "\t\t\tedge_index_by_type[(\"album\", \"includes\", \"track\")] += [(t_id, s_id)]\n",
    "\t\telif G[edge[0]][edge[1]][\"edge_type\"] == \"track-artist\":\n",
    "\t\t\ts_id = node_id(\"track\", edge[0], exception=True)\n",
    "\t\t\tt_id = node_id(\"artist\", edge[1], exception=True)\n",
    "\t\t\t\n",
    "\t\t\tedge_index_by_type[(\"artist\", \"authors\", \"track\")] += [(t_id, s_id)]\n",
    "\n",
    "\t# construct HeteroData\n",
    "\thetero = torch_geometric.data.HeteroData()\n",
    "\n",
    "\t# add initial node features\n",
    "\thetero[\"playlist\"].x = torch.FloatTensor(node_features_by_type[\"playlist\"])\n",
    "\thetero[\"track\"].x = torch.FloatTensor(node_features_by_type[\"track\"])\n",
    "\thetero[\"artist\"].x = torch.FloatTensor(node_features_by_type[\"artist\"])\n",
    "\thetero[\"album\"].x = torch.FloatTensor(node_features_by_type[\"album\"])\t\n",
    "\t\n",
    "\t# add edge indices\n",
    "\thetero[\"playlist\", \"contains\", \"track\"].edge_index = torch.tensor(edge_index_by_type[(\"playlist\", \"contains\", \"track\")]).t()\n",
    "\thetero[\"album\", \"includes\", \"track\"].edge_index = torch.tensor(edge_index_by_type[(\"album\", \"includes\", \"track\")]).t()\n",
    "\thetero[\"artist\", \"authors\", \"track\"].edge_index = torch.tensor(edge_index_by_type[(\"artist\", \"authors\", \"track\")]).t()\n",
    "\n",
    "\treturn hetero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "node spotify:album:0vlAYzvBDkRrRFpmR4v5MF not found for type track",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m graph_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pickles, \u001b[39m\"\u001b[39m\u001b[39mtop-G-500.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m G \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(graph_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m our_data \u001b[39m=\u001b[39m nx2hetero(G)\n",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m, in \u001b[0;36mnx2hetero\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     56\u001b[0m \tedge_index_by_type[(\u001b[39m\"\u001b[39m\u001b[39mplaylist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontains\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrack\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [(t_id, s_id)]\n\u001b[1;32m     57\u001b[0m \u001b[39melif\u001b[39;00m G[edge[\u001b[39m0\u001b[39m]][edge[\u001b[39m1\u001b[39m]][\u001b[39m\"\u001b[39m\u001b[39medge_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrack-album\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m \ts_id \u001b[39m=\u001b[39m node_id(\u001b[39m\"\u001b[39;49m\u001b[39mtrack\u001b[39;49m\u001b[39m\"\u001b[39;49m, edge[\u001b[39m0\u001b[39;49m], exception\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     59\u001b[0m \tt_id \u001b[39m=\u001b[39m node_id(\u001b[39m\"\u001b[39m\u001b[39malbum\u001b[39m\u001b[39m\"\u001b[39m, edge[\u001b[39m1\u001b[39m], exception\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \tedge_index_by_type[(\u001b[39m\"\u001b[39m\u001b[39malbum\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mincludes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrack\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [(t_id, s_id)]\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mnx2hetero.<locals>.node_id\u001b[0;34m(type, id, exception)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m d:\n\u001b[1;32m     18\u001b[0m \t\u001b[39mif\u001b[39;00m exception:\n\u001b[0;32m---> 19\u001b[0m \t\t\u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m not found for type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \td[\u001b[39mid\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(d)\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m d[\u001b[39mid\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: node spotify:album:0vlAYzvBDkRrRFpmR4v5MF not found for type track"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "base = \"spotify_million_playlist_dataset\"\n",
    "pickles = base + \"/pickles\"\n",
    "graph_path = os.path.join(pickles, \"top-G-500.pkl\")\n",
    "\n",
    "G = pickle.load(open(graph_path, \"rb\"))\n",
    "\n",
    "our_data = nx2hetero(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(nx.connected_components(G.to_undirected())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training mask for playlist nodes\n",
    "train_mask = torch.zeros(our_data[\"playlist\"].x.shape[0], dtype=torch.bool)\n",
    "train_mask[torch.randperm(train_mask.shape[0])[:int(train_mask.shape[0]*0.8)]] = True\n",
    "\n",
    "our_data[\"playlist\"].train_mask = train_mask\n",
    "\n",
    "our_data[\"playlist\"].y = torch.LongTensor([1]*our_data[\"playlist\"].x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   515,   664,  ...,   999,   999,   999],\n",
       "        [    0,     0,     0,  ..., 35286, 35287, 35288]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data[\"playlist\", \"contains\", \"track\"].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['playlist', 'track', 'artist', 'album'],\n",
       " [('playlist', 'contains', 'track'),\n",
       "  ('album', 'includes', 'track'),\n",
       "  ('artist', 'authors', 'track')])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = our_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mplaylist\u001b[0m={\n",
       "    x=[1000, 6],\n",
       "    train_mask=[1000],\n",
       "    y=[1000]\n",
       "  },\n",
       "  \u001b[1mtrack\u001b[0m={ x=[35289, 6] },\n",
       "  \u001b[1martist\u001b[0m={ x=[10091, 6] },\n",
       "  \u001b[1malbum\u001b[0m={ x=[20469, 6] },\n",
       "  \u001b[1m(playlist, contains, track)\u001b[0m={ edge_index=[2, 66331] },\n",
       "  \u001b[1m(album, includes, track)\u001b[0m={ edge_index=[2, 35289] },\n",
       "  \u001b[1m(artist, authors, track)\u001b[0m={ edge_index=[2, 35289] }\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 136909], x=[66849, 6], train_mask=[66849], y=[66849], node_type=[66849], edge_type=[136909])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homogeneous_data = data.to_homogeneous()\n",
    "homogeneous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mplaylist\u001b[0m={\n",
       "    x=[1000, 6],\n",
       "    train_mask=[1000],\n",
       "    y=[1000]\n",
       "  },\n",
       "  \u001b[1mtrack\u001b[0m={ x=[35289, 6] },\n",
       "  \u001b[1martist\u001b[0m={ x=[10091, 6] },\n",
       "  \u001b[1malbum\u001b[0m={ x=[20469, 6] },\n",
       "  \u001b[1m(playlist, contains, track)\u001b[0m={ edge_index=[2, 66331] },\n",
       "  \u001b[1m(album, includes, track)\u001b[0m={ edge_index=[2, 35289] },\n",
       "  \u001b[1m(artist, authors, track)\u001b[0m={ edge_index=[2, 35289] }\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "if not data.is_undirected():\n",
    "    data = T.ToUndirected()(data)\n",
    "# data = T.NormalizeFeatures()(data)\n",
    "if data.has_isolated_nodes():\n",
    "    data = T.RemoveIsolatedNodes()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_undirected(), data.has_isolated_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels, normalize=True)\n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def forward(self, x_playlist, x_track, playlist_track_edge):\n",
    "        playlist_embedding = x_playlist[playlist_track_edge[0]]\n",
    "        track_embedding = x_track[playlist_track_edge[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (playlist_embedding * track_embedding).sum(dim=-1)\n",
    "\n",
    "class HeteroModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, node_features, metadata):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for users and movies:\n",
    "        \n",
    "        self.node_lin = {\n",
    "            k: torch.nn.Linear(v.shape[1], hidden_channels, bias=True) for k, v in node_features.items()\n",
    "        }\n",
    "        \n",
    "        for _, v in self.node_lin.items():\n",
    "            torch.nn.init.xavier_uniform_(v.weight)\n",
    "\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=metadata)\n",
    "\n",
    "        self.classifier = LinkPredictor()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict = {\n",
    "            k: self.node_lin[k](v) for k, v in data.x_dict.items()\n",
    "        }\n",
    "        \n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"playlist\"],\n",
    "            x_dict[\"track\"],\n",
    "            data[\"playlist\", \"contains\", \"track\"].edge_label_index,\n",
    "        )\n",
    "        return pred\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for _, v in self.node_lin.items():\n",
    "            torch.nn.init.xavier_uniform_(v.weight)\n",
    "        self.gnn.reset_parameters()\n",
    "\n",
    "\n",
    "model = HeteroModel(256, data.x_dict, data.metadata())\n",
    "# model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    disjoint_train_ratio=0.3,\n",
    "    neg_sampling_ratio=2.0,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"playlist\", \"contains\", \"track\"),\n",
    "    rev_edge_types=(\"track\", \"rev_contains\", \"playlist\"), \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "edge_label_index = train_data[\"playlist\", \"contains\", \"track\"].edge_label_index\n",
    "edge_label = train_data[\"playlist\", \"contains\", \"track\"].edge_label\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio=2.0,\n",
    "    edge_label_index=((\"playlist\", \"contains\", \"track\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "\n",
    "    sample_outputs = []\n",
    "    for batch in tqdm(train_loader, desc='Training'):\n",
    "        optimizer.zero_grad()\n",
    "        # batch = batch.to('cuda:0')\n",
    "        out = model(batch)\n",
    "        \n",
    "        sample_outputs.append(round(out[0].item(), 2))\n",
    "        loss = F.cross_entropy(out,\n",
    "                               batch[\"playlist\", \"contains\", \"track\"].edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_examples += len(out)\n",
    "        total_loss += float(loss) * len(out)\n",
    "\n",
    "    return total_loss / total_examples, sample_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cpu'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:25<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:25<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:24<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:25<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:24<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.6366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:24<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:24<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.6390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:24<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:24<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.6383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 125/125 [00:25<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████                      | 68/125 [00:13<00:11,  4.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m ground_truth \u001b[39m=\u001b[39m sampled_data[\u001b[39m\"\u001b[39m\u001b[39mplaylist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontains\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrack\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39medge_label\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(pred, ground_truth)\n\u001b[0;32m---> 18\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss) \u001b[39m*\u001b[39m pred\u001b[39m.\u001b[39mnumel()\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib64/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib64/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(f\"Device: '{device}'\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.005)\n",
    "for epoch in range(1, 100):\n",
    "    total_loss = total_examples = 0\n",
    "    for sampled_data in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        sampled_data.to(device)\n",
    "        pred = model(sampled_data)\n",
    "        ground_truth = sampled_data[\"playlist\", \"contains\", \"track\"].edge_label\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.numel()\n",
    "        total_examples += pred.numel()\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation seed edges:\n",
    "edge_label_index = val_data[\"playlist\", \"contains\", \"track\"].edge_label_index\n",
    "edge_label = val_data[\"playlist\", \"contains\", \"track\"].edge_label\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"playlist\", \"contains\", \"track\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "sampled_data = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 52/52 [00:01<00:00, 28.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm.tqdm(val_loader):\n",
    "    with torch.no_grad():\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[\"playlist\", \"contains\", \"track\"].edge_label)\n",
    "pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "auc = roc_auc_score(ground_truth, pred)\n",
    "print()\n",
    "print(f\"Validation AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
