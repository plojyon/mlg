{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this project, we will train a GNN to perform link prediction on a heterogenous graph from the Spotify Million Playlists dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys  \n",
        "sys.path.insert(0, '/home/yon/jupyter-server/mlg/src/')\n",
        "\n",
        "import loader\n",
        "import config\n",
        "import model as M\n",
        "import preprocessing\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import random\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torcheval.metrics import BinaryAccuracy\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch_geometric.nn.SAGEConv((-1, -1), hidden_channels, normalize=True, dropout=True)\n",
        "        self.conv2 = torch_geometric.nn.SAGEConv((-1, -1), hidden_channels, normalize=True, dropout=True)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        return x\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "        self.conv3.reset_parameters()\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def forward(self, x_track, x_playlist, track_playlist_edge):\n",
        "        track_embedding = x_track[track_playlist_edge[0]]\n",
        "        playlist_embedding = x_playlist[track_playlist_edge[1]]\n",
        "\n",
        "        #print(playlist_embedding)\n",
        "\n",
        "        # Apply dot-product to get a prediction per supervision edge:\n",
        "        return (playlist_embedding * track_embedding).sum(dim=-1)\n",
        "\n",
        "class HeteroModel(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, node_features, metadata):\n",
        "        super().__init__()\n",
        "        # Since the dataset does not come with rich features, we also learn two\n",
        "        # embedding matrices for users and movies:\n",
        "\n",
        "        self.node_lin = {\n",
        "            k: torch.nn.Linear(v.shape[1], hidden_channels).to(device) for k, v in node_features.items()\n",
        "        }\n",
        "        \n",
        "        # Instantiate homogeneous GNN:\n",
        "        self.gnn = GNN(hidden_channels).to(device)\n",
        "        # Convert GNN model into a heterogeneous variant:\n",
        "        self.gnn = torch_geometric.nn.to_hetero(self.gnn, metadata=metadata).to(device)\n",
        "\n",
        "        self.classifier = LinkPredictor().to(device)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_dict = {\n",
        "            k: self.node_lin[k](v) for k, v in data.x_dict.items()\n",
        "        }\n",
        "\n",
        "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "        pred = self.classifier(\n",
        "            x_dict[\"track\"],\n",
        "            x_dict[\"playlist\"],\n",
        "            data[\"track\", \"contains\", \"playlist\"].edge_label_index,\n",
        "        )\n",
        "        return pred\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for _, v in self.node_lin.items():\n",
        "            torch.nn.init.xavier_uniform_(v.weight)\n",
        "        self.gnn.reset_parameters()\n",
        "\n",
        "def dummy_generator(source):\n",
        "    for e in source:\n",
        "        yield e\n",
        "\n",
        "outs = []\n",
        "\n",
        "def train(model, train_loader, optimizer, batch_wrapper=dummy_generator):\n",
        "    model.train()\n",
        "\n",
        "    accuracy = 0\n",
        "\n",
        "    total_examples = total_loss = 0\n",
        "    for i, batch in enumerate(batch_wrapper(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        out = model(batch)\n",
        "        truth = batch[\"track\", \"contains\", \"playlist\"].edge_label\n",
        "\n",
        "\n",
        "        ind = torch.randint(len(out),(5,))\n",
        "\n",
        "        if(i % 10 == 0):\n",
        "            #print(out[:10])\n",
        "            #print(batch[\"track\", \"contains\", \"playlist\"].edge_label[:10])\n",
        "            pass\n",
        "        loss = torch.nn.functional.mse_loss(\n",
        "            out, truth\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metric = BinaryAccuracy()\n",
        "\n",
        "        aute_gledam = out.to('cpu')\n",
        "\n",
        "        outs.append(aute_gledam)\n",
        "\n",
        "        metric.update(aute_gledam, truth.to('cpu'))\n",
        "        accuracy += metric.compute() * len(out)\n",
        "\n",
        "        total_examples += len(out)\n",
        "        total_loss += float(loss) * len(out)\n",
        "\n",
        "    return total_loss / total_examples, accuracy / total_examples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ghetero from pickle ...\n",
            "Loading datasets from pickle ...\n"
          ]
        }
      ],
      "source": [
        "ghetero = loader.get_ghetero(False, config)\n",
        "data_train, data_val, data_test = loader.get_datasets(False, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train100%|██████████| 500/500 [00:22<00:00, 22.06epoch/s, Loss: 0.1890, Accuracy 0.7169]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# create training mask for playlist nodes\n",
        "train_mask = torch.zeros(ghetero[\"playlist\"].x.shape[0], dtype=torch.bool)\n",
        "train_mask[torch.randperm(train_mask.shape[0])[:int(train_mask.shape[0]*0.8)]] = True\n",
        "\n",
        "ghetero[\"playlist\"].train_mask = train_mask\n",
        "\n",
        "ghetero[\"playlist\"].y = torch.LongTensor([1]*ghetero[\"playlist\"].x.shape[0]).to(device)\n",
        "\n",
        "model = HeteroModel(64, ghetero.x_dict, ghetero.metadata()).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.000001)\n",
        "edge_label_index = data_train[\"track\", \"contains\", \"playlist\"].edge_label_index\n",
        "edge_label = data_train[\"track\", \"contains\", \"playlist\"].edge_label\n",
        "\n",
        "def kek(*k,**k1):\n",
        "    print(\"to device called\", k, k1)\n",
        "    return T.ToDevice(device)(*k,**k1)\n",
        "\n",
        "train_loader = torch_geometric.loader.LinkNeighborLoader(\n",
        "    data=data_train,\n",
        "    num_neighbors=[-1],\n",
        "    neg_sampling_ratio=0.5,\n",
        "    edge_label_index=((\"track\", \"contains\", \"playlist\"), edge_label_index),\n",
        "    edge_label=edge_label,\n",
        "    batch_size=20000,\n",
        "    shuffle=True,\n",
        "    transform=T.ToDevice(device)\n",
        ")\n",
        "\n",
        "import tqdm\n",
        "epoch = 2000\n",
        "render_graph = True\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "epoch_iter = tqdm.tqdm(range(epoch), unit='epoch', desc='Training', bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:10}{r_bar}')\n",
        "for i in epoch_iter:\n",
        "    loss, accuracy = train(model, train_loader, optimizer)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(accuracy)\n",
        "    epoch_iter.set_postfix_str(f\"Loss: {loss:.4f}, Accuracy {accuracy:.4f}\")\n",
        "\n",
        "plt.clf()\n",
        "# add labels\n",
        "plt.plot(np.arange(len(accuracies)), accuracies, label='Accuracy')\n",
        "plt.plot(np.arange(len(losses)), losses, label='Loss')\n",
        "# add legend\n",
        "\n",
        "#start plot at 0\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([0.1857, 0.1595, 0.1615,  ..., 0.1488, 0.1523, 0.1577],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.1892, 0.1916, 0.2759,  ..., 0.2757, 0.1809, 0.1843],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.1593, 0.2216, 0.3024,  ..., 0.2177, 0.2259, 0.3055],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.2622, 0.2485, 0.3336,  ..., 0.2610, 0.2426, 0.2536],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.2800, 0.3754, 0.4073,  ..., 0.2980, 0.2994, 0.3038],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.2458, 0.4130, 0.3308,  ..., 0.3214, 0.3242, 0.4042],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3496, 0.4433, 0.3489,  ..., 0.4458, 0.3723, 0.3954],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3995, 0.4020, 0.3963,  ..., 0.4278, 0.4310, 0.5229],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4377, 0.5127, 0.5197,  ..., 0.4699, 0.3397, 0.4292],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4808, 0.4817, 0.5335,  ..., 0.5658, 0.5463, 0.4556],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5386, 0.5855, 0.5835,  ..., 0.5900, 0.5881, 0.5852],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4651, 0.5349, 0.6234,  ..., 0.6338, 0.6326, 0.5593],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4669, 0.4799, 0.5965,  ..., 0.5032, 0.6438, 0.5942],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6805, 0.6121, 0.6225,  ..., 0.6946, 0.6183, 0.6314],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7139, 0.7259, 0.6028,  ..., 0.7317, 0.6694, 0.5840],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7477, 0.7444, 0.6945,  ..., 0.6800, 0.7304, 0.6820],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6359, 0.7590, 0.7784,  ..., 0.6145, 0.6773, 0.7113],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7405, 0.7379, 0.7452,  ..., 0.7826, 0.7351, 0.7345],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6658, 0.7564, 0.7210,  ..., 0.7983, 0.8045, 0.6766],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7806, 0.7703, 0.7847,  ..., 0.8231, 0.6532, 0.7783],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7781, 0.8264, 0.8140,  ..., 0.7961, 0.7736, 0.7721],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7211, 0.7446, 0.8208,  ..., 0.7814, 0.7787, 0.8450],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7537, 0.6924, 0.8190,  ..., 0.7867, 0.7791, 0.7769],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7749, 0.8192, 0.7070,  ..., 0.6717, 0.8178, 0.6861],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8334, 0.6932, 0.8213,  ..., 0.7761, 0.7610, 0.7401],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6860, 0.7492, 0.7685,  ..., 0.7570, 0.7429, 0.8110],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7952, 0.7440, 0.7561,  ..., 0.8212, 0.6791, 0.8027],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8015, 0.7296, 0.8019,  ..., 0.7944, 0.8135, 0.7246],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7267, 0.7855, 0.7206,  ..., 0.7364, 0.7205, 0.7153],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7874, 0.7769, 0.6291,  ..., 0.7817, 0.6232, 0.7867],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6800, 0.6846, 0.6883,  ..., 0.7804, 0.6860, 0.7742],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6700, 0.7492, 0.6155,  ..., 0.7676, 0.6164, 0.6615],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6636, 0.7529, 0.7268,  ..., 0.6474, 0.6772, 0.6664],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6520, 0.6364, 0.7347,  ..., 0.6620, 0.5930, 0.6203],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7338, 0.6284, 0.6880,  ..., 0.7132, 0.7238, 0.6021],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7233, 0.6335, 0.6433,  ..., 0.7425, 0.6053, 0.6317],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6936, 0.6605, 0.6996,  ..., 0.7102, 0.5970, 0.5610],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6188, 0.7210, 0.7076,  ..., 0.6173, 0.7063, 0.6510],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7141, 0.6864, 0.6864,  ..., 0.5880, 0.5494, 0.6100],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6946, 0.5696, 0.5850,  ..., 0.5975, 0.6696, 0.5922],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6990, 0.6920, 0.6780,  ..., 0.6814, 0.6895, 0.6788],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5711, 0.5947, 0.5588,  ..., 0.6665, 0.6965, 0.5801],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5999, 0.6133, 0.5994,  ..., 0.6673, 0.5875, 0.5346],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6084, 0.6764, 0.5623,  ..., 0.5767, 0.6779, 0.5816],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6943, 0.6791, 0.5844,  ..., 0.5806, 0.6058, 0.7194],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6856, 0.6079, 0.6779,  ..., 0.5834, 0.5985, 0.6012],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7043, 0.7218, 0.5904,  ..., 0.6043, 0.5853, 0.5857],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6782, 0.6351, 0.6986,  ..., 0.6074, 0.6100, 0.7099],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7086, 0.6915, 0.6932,  ..., 0.6038, 0.7113, 0.6694],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6106, 0.7170, 0.5936,  ..., 0.6088, 0.5786, 0.7171],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6991, 0.6254, 0.7031,  ..., 0.6692, 0.6051, 0.6299],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6386, 0.5897, 0.6372,  ..., 0.7293, 0.6084, 0.7116],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6422, 0.7337, 0.7217,  ..., 0.5905, 0.7292, 0.7224],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6344, 0.6415, 0.6309,  ..., 0.6427, 0.7366, 0.7101],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6382, 0.6353, 0.6482,  ..., 0.7334, 0.6477, 0.6450],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7434, 0.6553, 0.7327,  ..., 0.7371, 0.6515, 0.6292],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7463, 0.6051, 0.7466,  ..., 0.6073, 0.7198, 0.6431],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7124, 0.6549, 0.6564,  ..., 0.6263, 0.7231, 0.7472],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7276, 0.6479, 0.6518,  ..., 0.7408, 0.6397, 0.6251],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6484, 0.6543, 0.6441,  ..., 0.7355, 0.7492, 0.7450],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7335, 0.7562, 0.7313,  ..., 0.6674, 0.6300, 0.7103],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6469, 0.6216, 0.6511,  ..., 0.6306, 0.6383, 0.6029],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7470, 0.6560, 0.6183,  ..., 0.6328, 0.6056, 0.6229],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6332, 0.6310, 0.7497,  ..., 0.6009, 0.6514, 0.6528],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6059, 0.7364, 0.7671,  ..., 0.6424, 0.6445, 0.6320],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7412, 0.5832, 0.6517,  ..., 0.6485, 0.7486, 0.7459],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7069, 0.7394, 0.7153,  ..., 0.5722, 0.5936, 0.6460],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7480, 0.6473, 0.6431,  ..., 0.7455, 0.7136, 0.7267],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7362, 0.7260, 0.5699,  ..., 0.6924, 0.6465, 0.7361],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7409, 0.7089, 0.6428,  ..., 0.6251, 0.5961, 0.5742],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6265, 0.7286, 0.7110,  ..., 0.7285, 0.5853, 0.6398],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5663, 0.7109, 0.6384,  ..., 0.7225, 0.6291, 0.6103],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7222, 0.5983, 0.6910,  ..., 0.6301, 0.5611, 0.6888],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6572, 0.7240, 0.6360,  ..., 0.6137, 0.7172, 0.6399],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6345, 0.6358, 0.6295,  ..., 0.7193, 0.6284, 0.7318],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6367, 0.7219, 0.6289,  ..., 0.7306, 0.6012, 0.7064],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7052, 0.7286, 0.6347,  ..., 0.6176, 0.6015, 0.6442],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6244, 0.7408, 0.6817,  ..., 0.7241, 0.7261, 0.7307],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6182, 0.6169, 0.7479,  ..., 0.7176, 0.7002, 0.6242],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6238, 0.6374, 0.6216,  ..., 0.6438, 0.6248, 0.7166],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6239, 0.7257, 0.7052,  ..., 0.7040, 0.5747, 0.6304],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6190, 0.5871, 0.6165,  ..., 0.6079, 0.6427, 0.5940],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6195, 0.7095, 0.7266,  ..., 0.7314, 0.6545, 0.6224],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5897, 0.6581, 0.5780,  ..., 0.5737, 0.6378, 0.6322],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6257, 0.6268, 0.7192,  ..., 0.6077, 0.5971, 0.6273],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7179, 0.7278, 0.6328,  ..., 0.5659, 0.5655, 0.7183],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7411, 0.7260, 0.7153,  ..., 0.6259, 0.6148, 0.5860],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7403, 0.6413, 0.7352,  ..., 0.6372, 0.7479, 0.6275],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6548, 0.6479, 0.6184,  ..., 0.7278, 0.6438, 0.6148],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7340, 0.7316, 0.5685,  ..., 0.6378, 0.7368, 0.6049],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7238, 0.7315, 0.5946,  ..., 0.7133, 0.7359, 0.7295],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6493, 0.7184, 0.6425,  ..., 0.6007, 0.7389, 0.7372],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7335, 0.7241, 0.6309,  ..., 0.6202, 0.7306, 0.6506],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6385, 0.6458, 0.6465,  ..., 0.7375, 0.6264, 0.7373],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6273, 0.6458, 0.6245,  ..., 0.7204, 0.6520, 0.6558],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7202, 0.7306, 0.5979,  ..., 0.5719, 0.7377, 0.7255],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7127, 0.7103, 0.5828,  ..., 0.7252, 0.7373, 0.7349],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7050, 0.7327, 0.5928,  ..., 0.6040, 0.7371, 0.5663],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7205, 0.6010, 0.6360,  ..., 0.6092, 0.6154, 0.6355],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6291, 0.7175, 0.7309,  ..., 0.6371, 0.6167, 0.5926],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6524, 0.7205, 0.7162,  ..., 0.6310, 0.7094, 0.6480],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6422, 0.7234, 0.7266,  ..., 0.6199, 0.5516, 0.5880],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7079, 0.7095, 0.6403,  ..., 0.6311, 0.7330, 0.6358],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6305, 0.7227, 0.5534,  ..., 0.6113, 0.7216, 0.5831],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7161, 0.7144, 0.7067,  ..., 0.7328, 0.6123, 0.5863],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6502, 0.6385, 0.5930,  ..., 0.4983, 0.7218, 0.7171],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6650, 0.7288, 0.6365,  ..., 0.6471, 0.6480, 0.7319],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6504, 0.6494, 0.6572,  ..., 0.6554, 0.7111, 0.6982],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7301, 0.6558, 0.6392,  ..., 0.6103, 0.6467, 0.7077],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7144, 0.5835, 0.6281,  ..., 0.5327, 0.6345, 0.6456],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7348, 0.6326, 0.6384,  ..., 0.6490, 0.7310, 0.7289],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6241, 0.5988, 0.7496,  ..., 0.6612, 0.6077, 0.6134],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6500, 0.6195, 0.7291,  ..., 0.5521, 0.6509, 0.7309],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7261, 0.7178, 0.6491,  ..., 0.7326, 0.7322, 0.5207],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7207, 0.7254, 0.7064,  ..., 0.6995, 0.6406, 0.6427],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6431, 0.7314, 0.5947,  ..., 0.5727, 0.7299, 0.5798],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6024, 0.5741, 0.6604,  ..., 0.5709, 0.5524, 0.5256],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7317, 0.7505, 0.5757,  ..., 0.6481, 0.6420, 0.7206],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7318, 0.7305, 0.6502,  ..., 0.6506, 0.6450, 0.7299],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6524, 0.6113, 0.6542,  ..., 0.5920, 0.6438, 0.7372],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6439, 0.5965, 0.7304,  ..., 0.6270, 0.7298, 0.5561],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7310, 0.7303, 0.6694,  ..., 0.5164, 0.7262, 0.6596],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6456, 0.7199, 0.5642,  ..., 0.5970, 0.5801, 0.5474],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7216, 0.7273, 0.5553,  ..., 0.6227, 0.5891, 0.6544],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6546, 0.7296, 0.5880,  ..., 0.6257, 0.5478, 0.6333],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7259, 0.6724, 0.7107,  ..., 0.6229, 0.6302, 0.5688],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7156, 0.7092, 0.7224,  ..., 0.5865, 0.7235, 0.7474],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5371, 0.7324, 0.6351,  ..., 0.6427, 0.6240, 0.5451],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6501, 0.7293, 0.5904,  ..., 0.7222, 0.7189, 0.7323],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7247, 0.6503, 0.6590,  ..., 0.7471, 0.6067, 0.7209],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6481, 0.6623, 0.7296,  ..., 0.7264, 0.6340, 0.5862],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6270, 0.6537, 0.6379,  ..., 0.6531, 0.7311, 0.7303],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7323, 0.6522, 0.7319,  ..., 0.7293, 0.6195, 0.6135],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7287, 0.7314, 0.6507,  ..., 0.5965, 0.5721, 0.7349],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7252, 0.7243, 0.7321,  ..., 0.6179, 0.6059, 0.5725],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6458, 0.7195, 0.6465,  ..., 0.6632, 0.5448, 0.7301],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7309, 0.5403, 0.6544,  ..., 0.5973, 0.5560, 0.7307],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6508, 0.5382, 0.7259,  ..., 0.6549, 0.7285, 0.7169],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5561, 0.6468, 0.5929,  ..., 0.5683, 0.5501, 0.7201],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6269, 0.6706, 0.7215,  ..., 0.5381, 0.6182, 0.5972],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6324, 0.7273, 0.7239,  ..., 0.7262, 0.6613, 0.7224],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6535, 0.6350, 0.7182,  ..., 0.6041, 0.7227, 0.7285],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7450, 0.5940, 0.7282,  ..., 0.6294, 0.6074, 0.7303],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6560, 0.7293, 0.7308,  ..., 0.7419, 0.7303, 0.6599],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6520, 0.7309, 0.7299,  ..., 0.5850, 0.4918, 0.5308],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7310, 0.6056, 0.7221,  ..., 0.6598, 0.5974, 0.6397],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7283, 0.6003, 0.6530,  ..., 0.7355, 0.6008, 0.6119],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5938, 0.6698, 0.7342,  ..., 0.6313, 0.6034, 0.6507],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7278, 0.6371, 0.7076,  ..., 0.5964, 0.4821, 0.5606],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6367, 0.6604, 0.7316,  ..., 0.7241, 0.6472, 0.7197],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7212, 0.7267, 0.6531,  ..., 0.6298, 0.7337, 0.7297],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7293, 0.7313, 0.7199,  ..., 0.6334, 0.6069, 0.7297],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6393, 0.7283, 0.7304,  ..., 0.5245, 0.7268, 0.7245],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7265, 0.6491, 0.6233,  ..., 0.5781, 0.5989, 0.6494],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7304, 0.6472, 0.5612,  ..., 0.7437, 0.7097, 0.5937],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7270, 0.6613, 0.6619,  ..., 0.6634, 0.6380, 0.5970],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7411, 0.6723, 0.6518,  ..., 0.5878, 0.6217, 0.6213],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5807, 0.6446, 0.7281,  ..., 0.7047, 0.5123, 0.7445],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6745, 0.7308, 0.7224,  ..., 0.6015, 0.6409, 0.7234],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5986, 0.7300, 0.6105,  ..., 0.5278, 0.7268, 0.6392],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7210, 0.7300, 0.6640,  ..., 0.6078, 0.7315, 0.7264],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7202, 0.7419, 0.5284,  ..., 0.7372, 0.6226, 0.5423],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6293, 0.7279, 0.7234,  ..., 0.6570, 0.7232, 0.7091],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7193, 0.7271, 0.6580,  ..., 0.5674, 0.6082, 0.6306],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6290, 0.7327, 0.6591,  ..., 0.5908, 0.5895, 0.7196],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7282, 0.7315, 0.6441,  ..., 0.6524, 0.7212, 0.7241],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7247, 0.6455, 0.7349,  ..., 0.6728, 0.6410, 0.6986],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7284, 0.6522, 0.7269,  ..., 0.7050, 0.6445, 0.6026],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6521, 0.7407, 0.7281,  ..., 0.6602, 0.6481, 0.7246],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6564, 0.7233, 0.5866,  ..., 0.5924, 0.7280, 0.7280],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7256, 0.6018, 0.7299,  ..., 0.5937, 0.6299, 0.6151],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6653, 0.6596, 0.6636,  ..., 0.6262, 0.4802, 0.7221],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5957, 0.7313, 0.7168,  ..., 0.6171, 0.5679, 0.5315],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7267, 0.6616, 0.6383,  ..., 0.6977, 0.5058, 0.6482],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7324, 0.7100, 0.7336,  ..., 0.7182, 0.6379, 0.7205],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6590, 0.7282, 0.7257,  ..., 0.5656, 0.6065, 0.5897],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7351, 0.6667, 0.6216,  ..., 0.6559, 0.7266, 0.6212],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7164, 0.7330, 0.7222,  ..., 0.7002, 0.6468, 0.7120],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5504, 0.7252, 0.6383,  ..., 0.5852, 0.6554, 0.6596],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6435, 0.6007, 0.5809,  ..., 0.7320, 0.6415, 0.6453],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7367, 0.7348, 0.6165,  ..., 0.7086, 0.7156, 0.5531],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7197, 0.6671, 0.7046,  ..., 0.6409, 0.6856, 0.7082],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7430, 0.5964, 0.7037,  ..., 0.7211, 0.6656, 0.6496],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7388, 0.7346, 0.7313,  ..., 0.6647, 0.5827, 0.7112],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6614, 0.7321, 0.7316,  ..., 0.6074, 0.7164, 0.6968],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6512, 0.6498, 0.7305,  ..., 0.6863, 0.6434, 0.6027],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6716, 0.5405, 0.6136,  ..., 0.6221, 0.6068, 0.5814],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6426, 0.7368, 0.6523,  ..., 0.6712, 0.7283, 0.7356],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6517, 0.6692, 0.6367,  ..., 0.7343, 0.6196, 0.6610],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6553, 0.5852, 0.7342,  ..., 0.5377, 0.6062, 0.7396],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5629, 0.6596, 0.5853,  ..., 0.5983, 0.6390, 0.5824],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7281, 0.6151, 0.6411,  ..., 0.6580, 0.6840, 0.5614],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7232, 0.6851, 0.6272,  ..., 0.6571, 0.7261, 0.6315],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7394, 0.7394, 0.7463,  ..., 0.6826, 0.5343, 0.5802],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7256, 0.5878, 0.7298,  ..., 0.6327, 0.7354, 0.6151],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5720, 0.6207, 0.7199,  ..., 0.7016, 0.6707, 0.5771],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7394, 0.6157, 0.7300,  ..., 0.6423, 0.7359, 0.6622],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6739, 0.6197, 0.6575,  ..., 0.6648, 0.6564, 0.7496],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6732, 0.6836, 0.7079,  ..., 0.6726, 0.5720, 0.7183],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7324, 0.6727, 0.6659,  ..., 0.6573, 0.7058, 0.6263],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6323, 0.6348, 0.6591,  ..., 0.5100, 0.7156, 0.5211],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7237, 0.7063, 0.7404,  ..., 0.6683, 0.6422, 0.6575],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7389, 0.7357, 0.7281,  ..., 0.6234, 0.6649, 0.4521],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6854, 0.7493, 0.6841,  ..., 0.6022, 0.6577, 0.6730],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6077, 0.5425, 0.6026,  ..., 0.5525, 0.5322, 0.6500],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5347, 0.7104, 0.7488,  ..., 0.5973, 0.6318, 0.5753],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6500, 0.7388, 0.7452,  ..., 0.5000, 0.5945, 0.6190],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6766, 0.6346, 0.5714,  ..., 0.7327, 0.4947, 0.6137],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5786, 0.6818, 0.6917,  ..., 0.7110, 0.6269, 0.7331],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6050, 0.6377, 0.6634,  ..., 0.6363, 0.5528, 0.6055],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7507, 0.7240, 0.5727,  ..., 0.6256, 0.6561, 0.5954],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6184, 0.7268, 0.6582,  ..., 0.6243, 0.7262, 0.5551],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5396, 0.7408, 0.5629,  ..., 0.6712, 0.6563, 0.7195],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7008, 0.6584, 0.6766,  ..., 0.6815, 0.5897, 0.6526],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6406, 0.6953, 0.5515,  ..., 0.7527, 0.6599, 0.5511],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6888, 0.7357, 0.7310,  ..., 0.7204, 0.6564, 0.5388],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7178, 0.5749, 0.6476,  ..., 0.7251, 0.6456, 0.7574],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6153, 0.6633, 0.6581,  ..., 0.6323, 0.6792, 0.6790],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6517, 0.7406, 0.6941,  ..., 0.6240, 0.4615, 0.6948],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7620, 0.5273, 0.7587,  ..., 0.6598, 0.4844, 0.6378],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6451, 0.6317, 0.6678,  ..., 0.6790, 0.7095, 0.7502],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7413, 0.7184, 0.7426,  ..., 0.6941, 0.5342, 0.6705],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7648, 0.7519, 0.6978,  ..., 0.6314, 0.6271, 0.6551],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7654, 0.6829, 0.7275,  ..., 0.5533, 0.7734, 0.6392],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6079, 0.6994, 0.6597,  ..., 0.7414, 0.7594, 0.5503],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6088, 0.5801, 0.7425,  ..., 0.4350, 0.6085, 0.6230],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6868, 0.6975, 0.6277,  ..., 0.7675, 0.7190, 0.5739],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6661, 0.5120, 0.6905,  ..., 0.6698, 0.6574, 0.6031],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7136, 0.5610, 0.6907,  ..., 0.6770, 0.6133, 0.5292],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7755, 0.7723, 0.6539,  ..., 0.6215, 0.6602, 0.6188],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7168, 0.7511, 0.7674,  ..., 0.7452, 0.7306, 0.7274],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6682, 0.7414, 0.7395,  ..., 0.6795, 0.6037, 0.6951],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6187, 0.7430, 0.7510,  ..., 0.7624, 0.6834, 0.6973],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6840, 0.4901, 0.6852,  ..., 0.6481, 0.6724, 0.4827],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5945, 0.7683, 0.6829,  ..., 0.6766, 0.6208, 0.5044],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6356, 0.7062, 0.5311,  ..., 0.6327, 0.6575, 0.7085],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4527, 0.7464, 0.7160,  ..., 0.4610, 0.6721, 0.6990],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6432, 0.7020, 0.7421,  ..., 0.6722, 0.6662, 0.5559],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6441, 0.7753, 0.7597,  ..., 0.6752, 0.6721, 0.6851],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7568, 0.7233, 0.6703,  ..., 0.6265, 0.5536, 0.7131],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6663, 0.5987, 0.7716,  ..., 0.5054, 0.5237, 0.5263],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6717, 0.7600, 0.7557,  ..., 0.5702, 0.6677, 0.4028],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8119, 0.6879, 0.7214,  ..., 0.6162, 0.6006, 0.5298],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6412, 0.6637, 0.6189,  ..., 0.6884, 0.6201, 0.7598],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6454, 0.7500, 0.7816,  ..., 0.6547, 0.7297, 0.6916],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6655, 0.5616, 0.6644,  ..., 0.7138, 0.6485, 0.6257],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7090, 0.5704, 0.3908,  ..., 0.5673, 0.3777, 0.6404],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6583, 0.7740, 0.7584,  ..., 0.4520, 0.6866, 0.5889],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5717, 0.6484, 0.7166,  ..., 0.6077, 0.5645, 0.6882],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5030, 0.6378, 0.7464,  ..., 0.6379, 0.7018, 0.5376],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6908, 0.7112, 0.6924,  ..., 0.6207, 0.7899, 0.5417],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7183, 0.6642, 0.7712,  ..., 0.5580, 0.4781, 0.5078],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7090, 0.7502, 0.6712,  ..., 0.5218, 0.6817, 0.7583],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7042, 0.6538, 0.6236,  ..., 0.4660, 0.4734, 0.7792],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5850, 0.6772, 0.6071,  ..., 0.7306, 0.5308, 0.5924],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7311, 0.7215, 0.6657,  ..., 0.5509, 0.7374, 0.8087],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6079, 0.7975, 0.6666,  ..., 0.5358, 0.7308, 0.7045],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7923, 0.6922, 0.4726,  ..., 0.5920, 0.6541, 0.5576],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6536, 0.7506, 0.6817,  ..., 0.5469, 0.5857, 0.6934],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5560, 0.7641, 0.7432,  ..., 0.3813, 0.6646, 0.6121],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7407, 0.6998, 0.7205,  ..., 0.7396, 0.6682, 0.7129],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6627, 0.8114, 0.7854,  ..., 0.6602, 0.4852, 0.5214],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7620, 0.6906, 0.6710,  ..., 0.6366, 0.6623, 0.7757],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7437, 0.6987, 0.7976,  ..., 0.5449, 0.6782, 0.5849],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7635, 0.7611, 0.7253,  ..., 0.7297, 0.7654, 0.4879],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7116, 0.7500, 0.7539,  ..., 0.5608, 0.5789, 0.6756],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5960, 0.8362, 0.7035,  ..., 0.5660, 0.6876, 0.7103],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7141, 0.7160, 0.6642,  ..., 0.7050, 0.6461, 0.4852],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6562, 0.7496, 0.4980,  ..., 0.6522, 0.5702, 0.6892],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8132, 0.7555, 0.4492,  ..., 0.4157, 0.4165, 0.6686],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6892, 0.7581, 0.6550,  ..., 0.8065, 0.3006, 0.8137],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7546, 0.5621, 0.8399,  ..., 0.7657, 0.4863, 0.5138],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7911, 0.7202, 0.8147,  ..., 0.7509, 0.3991, 0.5016],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6410, 0.8238, 0.7725,  ..., 0.4364, 0.4701, 0.7849],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5744, 0.5426, 0.7622,  ..., 0.6343, 0.4437, 0.2615],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7768, 0.5518, 0.7238,  ..., 0.5003, 0.4044, 0.6550],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7389, 0.7039, 0.5933,  ..., 0.7622, 0.5890, 0.3406],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6588, 0.5306, 0.7767,  ..., 0.3899, 0.4875, 0.7614],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7802, 0.7568, 0.6949,  ..., 0.7637, 0.5542, 0.8100],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8395, 0.8059, 0.6802,  ..., 0.8420, 0.5561, 0.5108],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5617, 0.5506, 0.7627,  ..., 0.5269, 0.5739, 0.6561],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7916, 0.5537, 0.5965,  ..., 0.8179, 0.5837, 0.5715],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7106, 0.6363, 0.5221,  ..., 0.7719, 0.6838, 0.8476],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4407, 0.8055, 0.7889,  ..., 0.5170, 0.4504, 0.6384],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7038, 0.7455, 0.4737,  ..., 0.7871, 0.6800, 0.7136],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4458, 0.7899, 0.7888,  ..., 0.7526, 0.7023, 0.4555],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7486, 0.4919, 0.4402,  ..., 0.5915, 0.7138, 0.6076],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3278, 0.8138, 0.6448,  ..., 0.4745, 0.7432, 0.5405],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7663, 0.8518, 0.7171,  ..., 0.7247, 0.7000, 0.7588],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6666, 0.4608, 0.8048,  ..., 0.7557, 0.2985, 0.7456],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6441, 0.7259, 0.6456,  ..., 0.3748, 0.7429, 0.5216],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7855, 0.7329, 0.8092,  ..., 0.5730, 0.6509, 0.7515],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6978, 0.6237, 0.7296,  ..., 0.8108, 0.7452, 0.6875],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7947, 0.8261, 0.7946,  ..., 0.6422, 0.7115, 0.4640],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8321, 0.7485, 0.7536,  ..., 0.7244, 0.7898, 0.8097],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8125, 0.7505, 0.5424,  ..., 0.5285, 0.6246, 0.8296],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5461, 0.7591, 0.6588,  ..., 0.8147, 0.7013, 0.8268],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7524, 0.8142, 0.5635,  ..., 0.5683, 0.8376, 0.3267],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3562, 0.8111, 0.6844,  ..., 0.5802, 0.6722, 0.5037],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7613, 0.5518, 0.7400,  ..., 0.6992, 0.3530, 0.4232],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7167, 0.6919, 0.7087,  ..., 0.5190, 0.6443, 0.6521],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5683, 0.4968, 0.8207,  ..., 0.5517, 0.8102, 0.3817],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7090, 0.7965, 0.6899,  ..., 0.7262, 0.7522, 0.3298],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7600, 0.5066, 0.6509,  ..., 0.5138, 0.5080, 0.6461],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7728, 0.8253, 0.8198,  ..., 0.4891, 0.7071, 0.5415],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8191, 0.7355, 0.7206,  ..., 0.6797, 0.3548, 0.7184],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7523, 0.7106, 0.7370,  ..., 0.4148, 0.6674, 0.4178],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3327, 0.8092, 0.7998,  ..., 0.6283, 0.6887, 0.7512],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7964, 0.6849, 0.5115,  ..., 0.3539, 0.6602, 0.6603],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7216, 0.7058, 0.7336,  ..., 0.7475, 0.7301, 0.6698],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7258, 0.7374, 0.4915,  ..., 0.7365, 0.6762, 0.4184],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7705, 0.7646, 0.8165,  ..., 0.6844, 0.4123, 0.8166],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6714, 0.7509, 0.6054,  ..., 0.6913, 0.7766, 0.4406],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7024, 0.4186, 0.4973,  ..., 0.4645, 0.6269, 0.6772],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6594, 0.5939, 0.7173,  ..., 0.3889, 0.5559, 0.8287],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7755, 0.7473, 0.8397,  ..., 0.6556, 0.7379, 0.6446],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5636, 0.8010, 0.6703,  ..., 0.7152, 0.5821, 0.6627],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6599, 0.6557, 0.5957,  ..., 0.7571, 0.3779, 0.4818],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8307, 0.6705, 0.7113,  ..., 0.5895, 0.3810, 0.8290],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7669, 0.6560, 0.7202,  ..., 0.7276, 0.4627, 0.6869],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6536, 0.6932, 0.8039,  ..., 0.5870, 0.7287, 0.8175],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8159, 0.7121, 0.3930,  ..., 0.7050, 0.4846, 0.7382],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4710, 0.8013, 0.7880,  ..., 0.6973, 0.7878, 0.3571],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6913, 0.6643, 0.6795,  ..., 0.5697, 0.6607, 0.7026],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7410, 0.7059, 0.7543,  ..., 0.5515, 0.4049, 0.6661],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7958, 0.8101, 0.8301,  ..., 0.5990, 0.7200, 0.3512],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5006, 0.7263, 0.8001,  ..., 0.8111, 0.3642, 0.5848],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8011, 0.4473, 0.7428,  ..., 0.5487, 0.4795, 0.6985],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7198, 0.8234, 0.7896,  ..., 0.6068, 0.6749, 0.4327],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5626, 0.8240, 0.8268,  ..., 0.5767, 0.4322, 0.7288],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4777, 0.7317, 0.4429,  ..., 0.2969, 0.6506, 0.5054],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8107, 0.8085, 0.5019,  ..., 0.6724, 0.3911, 0.4763],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8201, 0.6076, 0.8222,  ..., 0.6648, 0.5368, 0.7339],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7913, 0.8213, 0.7698,  ..., 0.4902, 0.5602, 0.6904],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7806, 0.7239, 0.6994,  ..., 0.7512, 0.4823, 0.7994],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7967, 0.4913, 0.6899,  ..., 0.6833, 0.7159, 0.7120],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6047, 0.8127, 0.7025,  ..., 0.5716, 0.6262, 0.5378],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5557, 0.4585, 0.4911,  ..., 0.6696, 0.4556, 0.3702],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7003, 0.8050, 0.5899,  ..., 0.5729, 0.4813, 0.4203],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6829, 0.6349, 0.7205,  ..., 0.6067, 0.3179, 0.7928],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6428, 0.7477, 0.6865,  ..., 0.8160, 0.7479, 0.6371],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7154, 0.5896, 0.6830,  ..., 0.6426, 0.6252, 0.8129],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8072, 0.5583, 0.6197,  ..., 0.7990, 0.6399, 0.7511],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7403, 0.7316, 0.6586,  ..., 0.6528, 0.6545, 0.3529],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7293, 0.5392, 0.4751,  ..., 0.6905, 0.4751, 0.3146],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6525, 0.6646, 0.7082,  ..., 0.7274, 0.6603, 0.6902],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7436, 0.7528, 0.8015,  ..., 0.2973, 0.5094, 0.4591],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7227, 0.6991, 0.7748,  ..., 0.7846, 0.7066, 0.7689],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7646, 0.6664, 0.3641,  ..., 0.7111, 0.8119, 0.6972],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7708, 0.7480, 0.6356,  ..., 0.8297, 0.7011, 0.4455],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6670, 0.8429, 0.7783,  ..., 0.5650, 0.7734, 0.7172],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7990, 0.8070, 0.8098,  ..., 0.6744, 0.4445, 0.6444],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8262, 0.6683, 0.7209,  ..., 0.7986, 0.3866, 0.7609],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8128, 0.5877, 0.7978,  ..., 0.3777, 0.6773, 0.6978],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6064, 0.7318, 0.6792,  ..., 0.6547, 0.5211, 0.3799],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7147, 0.8187, 0.8009,  ..., 0.2245, 0.8111, 0.5457],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8038, 0.7284, 0.8143,  ..., 0.8196, 0.4522, 0.7387],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7362, 0.7150, 0.7998,  ..., 0.5768, 0.3480, 0.7707],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8307, 0.6906, 0.7829,  ..., 0.7396, 0.6477, 0.7313],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7379, 0.8237, 0.4363,  ..., 0.6756, 0.4113, 0.4045],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6020, 0.8159, 0.6229,  ..., 0.6850, 0.3925, 0.5091],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6516, 0.7981, 0.8117,  ..., 0.8079, 0.5905, 0.4528],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7459, 0.6028, 0.8661,  ..., 0.5567, 0.7092, 0.6441],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8079, 0.5118, 0.5939,  ..., 0.3963, 0.3505, 0.6020],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6848, 0.6794, 0.5457,  ..., 0.7946, 0.7100, 0.3451],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6604, 0.7807, 0.6399,  ..., 0.6726, 0.5993, 0.6421],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3604, 0.8268, 0.5380,  ..., 0.6182, 0.3988, 0.5032],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7833, 0.7082, 0.5938,  ..., 0.5048, 0.6836, 0.5845],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7393, 0.8970, 0.6285,  ..., 0.6746, 0.7771, 0.4207],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6203, 0.6647, 0.7030,  ..., 0.4737, 0.3006, 0.6541],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7987, 0.5807, 0.6408,  ..., 0.5566, 0.5507, 0.8157],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4135, 0.6918, 0.4879,  ..., 0.7178, 0.7881, 0.7513],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5839, 0.4038, 0.7984,  ..., 0.4074, 0.7284, 0.6422],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4814, 0.3799, 0.8668,  ..., 0.6983, 0.6372, 0.6512],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7412, 0.7027, 0.7365,  ..., 0.7002, 0.6970, 0.6864],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8159, 0.7980, 0.4117,  ..., 0.3827, 0.5931, 0.3911],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8081, 0.6795, 0.8018,  ..., 0.4449, 0.4798, 0.8027],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8428, 0.8284, 0.7387,  ..., 0.6965, 0.8078, 0.7328],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7068, 0.7364, 0.4070,  ..., 0.8086, 0.8174, 0.4104],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8250, 0.8695, 0.7773,  ..., 0.6728, 0.3931, 0.7154],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7339, 0.6682, 0.8892,  ..., 0.7962, 0.4195, 0.3917],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6445, 0.8754, 0.5892,  ..., 0.3683, 0.2477, 0.3524],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8048, 0.2993, 0.7125,  ..., 0.8229, 0.8208, 0.2406],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7628, 0.8204, 0.8870,  ..., 0.5509, 0.6051, 0.4176],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9526, 0.6556, 0.7955,  ..., 0.7942, 0.6129, 0.7608],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7284, 0.7657, 0.6790,  ..., 0.4938, 0.7112, 0.6550],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9442, 0.7042, 0.5981,  ..., 0.5426, 0.6785, 0.7421],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9330, 0.5383, 0.6818,  ..., 0.3792, 0.7220, 0.6252],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7629, 0.7860, 0.8181,  ..., 0.6170, 0.7866, 0.4665],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3903, 0.8152, 0.4376,  ..., 0.2021, 0.7361, 0.7055],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6424, 0.4703, 0.6822,  ..., 0.2922, 0.7402, 0.4682],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8170, 0.7978, 0.5273,  ..., 0.7018, 0.6247, 0.5843],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6106, 0.7109, 0.7406,  ..., 0.2633, 0.3251, 0.3051],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8256, 0.8202, 0.6300,  ..., 0.8222, 0.5203, 0.7269],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7986, 0.8207, 0.4171,  ..., 0.5373, 0.4444, 0.5266],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7793, 0.7918, 0.7967,  ..., 0.4164, 0.6842, 0.6146],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4919, 0.8087, 0.4656,  ..., 0.3598, 0.6048, 0.5457],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8100, 0.7876, 0.7015,  ..., 0.4774, 0.8054, 0.4599],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6963, 0.8020, 0.6346,  ..., 0.2683, 0.5216, 0.5891],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7573, 0.7682, 0.7165,  ..., 0.8239, 0.6019, 0.4174],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6660, 0.7561, 0.4093,  ..., 0.7673, 0.4270, 0.6933],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7614, 0.6802, 0.8418,  ..., 0.5221, 0.5181, 0.4588],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7066, 0.7934, 0.7874,  ..., 0.8179, 0.3513, 0.7289],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7980, 0.6964, 0.8151,  ..., 0.4858, 0.7086, 0.5234],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8085, 0.7828, 0.6277,  ..., 0.6611, 0.4325, 0.4474],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5749, 0.9189, 0.5303,  ..., 0.6733, 0.5109, 0.3366],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7647, 0.8333, 0.8158,  ..., 0.8210, 0.7105, 0.6838],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8173, 0.7328, 0.6913,  ..., 0.7822, 0.3332, 0.7585],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6242, 0.8371, 0.6239,  ..., 0.7822, 0.4216, 0.7803],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6392, 0.6672, 0.7542,  ..., 0.3519, 0.6792, 0.3541],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9754, 0.7792, 0.8051,  ..., 0.6028, 0.7406, 0.2905],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4779, 0.9153, 0.9473,  ..., 0.5813, 0.5388, 0.8156],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5959, 0.3922, 0.4308,  ..., 0.4935, 0.5679, 0.5137],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6408, 0.8121, 0.7007,  ..., 0.7196, 0.7344, 0.5125],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9015, 0.3946, 0.8882,  ..., 0.5732, 0.6215, 0.1882],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6770, 0.8460, 0.8174,  ..., 0.4703, 0.4067, 0.4290],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9682, 0.5822, 0.8073,  ..., 0.2656, 0.5541, 0.7959],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7542, 0.8014, 0.8200,  ..., 0.4354, 0.4705, 0.6143],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6584, 0.7714, 0.5496,  ..., 0.8008, 0.7101, 0.7581],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9257, 0.7199, 0.5125,  ..., 0.5339, 0.3465, 0.5461],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7788, 0.7965, 0.6687,  ..., 0.3846, 0.3674, 0.2910],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7588, 0.7322, 0.6448,  ..., 0.6212, 0.6150, 0.9032],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7014, 0.6499, 0.6759,  ..., 0.4791, 0.5523, 0.4310],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8063, 0.5943, 0.4510,  ..., 0.5227, 0.5152, 0.4444],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5283, 0.5628, 0.7328,  ..., 0.6208, 0.6547, 0.3375],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6589, 0.6956, 0.9981,  ..., 0.5418, 0.8135, 0.3444],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8414, 0.6079, 0.8134,  ..., 0.1788, 0.7948, 0.7321],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9436, 0.5800, 0.8098,  ..., 0.3612, 0.7141, 0.3935],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6186, 0.6945, 0.8081,  ..., 0.4926, 0.4108, 0.4829],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6661, 0.7302, 0.8392,  ..., 0.5114, 0.4026, 0.4762],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8201, 0.7506, 0.7853,  ..., 0.7245, 0.6203, 0.4480],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6718, 0.6801, 0.7478,  ..., 0.7292, 0.3190, 0.3644],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7982, 0.9326, 0.5374,  ..., 0.6474, 0.6695, 0.6434],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6198, 0.7628, 0.6381,  ..., 0.7077, 0.5088, 0.4189],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5600, 0.5732, 0.7245,  ..., 0.6132, 0.2803, 0.4251],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8134, 0.8671, 0.8171,  ..., 0.6582, 0.8013, 0.6394],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8161, 0.7096, 0.6651,  ..., 0.7752, 0.3107, 0.8014],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8266, 0.9046, 0.7146,  ..., 0.6298, 0.7761, 0.3710],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6430, 0.8293, 0.4007,  ..., 0.3173, 0.4905, 0.5558],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9298, 0.6734, 0.8038,  ..., 0.9231, 0.6843, 0.9084],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6783, 0.8082, 0.3733,  ..., 0.5820, 0.6054, 0.7172],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6959, 0.5865, 0.6998,  ..., 0.5458, 0.7954, 0.4459],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4528, 0.6717, 0.7732,  ..., 0.2376, 0.7362, 0.7251],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8697, 0.7141, 0.8070,  ..., 0.4806, 0.2607, 0.8028],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5892, 0.9007, 0.9506,  ..., 0.8053, 0.5489, 0.7395],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7528, 0.8738, 0.6232,  ..., 0.5120, 0.6845, 0.7568],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7546, 0.6144, 0.9076,  ..., 0.7692, 0.4153, 0.4063],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6871, 0.7405, 0.8161,  ..., 0.4998, 0.4034, 0.8159],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9645, 0.7712, 0.8163,  ..., 0.4296, 0.4188, 0.7496],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8238, 0.9025, 0.7135,  ..., 0.1734, 0.6996, 0.8018],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8636, 0.5986, 0.7706,  ..., 0.4187, 0.8149, 0.8052],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7760, 0.7071, 0.8003,  ..., 0.6036, 0.8158, 0.5055],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7193, 0.7546, 0.8249,  ..., 0.6959, 0.4491, 0.7106],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7832, 0.5389, 0.7134,  ..., 0.5369, 0.6323, 0.6365],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6822, 0.7576, 0.5737,  ..., 0.5467, 0.7420, 0.7809],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8144, 0.6392, 0.4234,  ..., 0.4955, 0.3731, 0.6006],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7889, 0.6698, 0.7640,  ..., 0.6727, 0.6897, 0.7426],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8032, 0.6891, 0.6504,  ..., 0.6561, 0.6089, 0.5544],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7118, 0.7949, 0.8011,  ..., 0.7852, 0.4670, 0.4273],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5635, 0.7487, 0.7728,  ..., 0.5451, 0.7044, 0.8254],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.2246, 0.8060, 0.6693,  ..., 0.7204, 0.5256, 0.3591],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7641, 0.8239, 0.8157,  ..., 0.2485, 0.4391, 0.5634],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6705, 0.8128, 0.9627,  ..., 0.8335, 0.6579, 0.7255],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6946, 0.5335, 0.6867,  ..., 0.7879, 0.6902, 0.7015],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3682, 0.7143, 0.4213,  ..., 0.3933, 0.6587, 0.6615],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7635, 0.4669, 0.5618,  ..., 0.3770, 0.5388, 0.5628],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6763, 0.4098, 0.5597,  ..., 0.5826, 0.7131, 0.4452],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3748, 0.7939, 0.8131,  ..., 0.3621, 0.3403, 0.4931],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8391, 0.7886, 0.7059,  ..., 0.5658, 0.6112, 0.1897],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7955, 0.6269, 0.5232,  ..., 0.7454, 0.5582, 0.4978],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5899, 0.5889, 0.6333,  ..., 0.5764, 0.6467, 0.5275],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6127, 0.5875, 0.6353,  ..., 0.7796, 0.7875, 0.5322],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7688, 0.7528, 0.7157,  ..., 0.7848, 0.4220, 0.4141],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7200, 0.6555, 0.5599,  ..., 0.6553, 0.6473, 0.3925],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5733, 0.5997, 0.8057,  ..., 0.3232, 0.4153, 0.3612],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6972, 0.5675, 0.8197,  ..., 0.7964, 0.4629, 0.3036],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7668, 0.7707, 0.7379,  ..., 0.2232, 0.7128, 0.4412],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8036, 0.6038, 0.8040,  ..., 0.6747, 0.5096, 0.6593],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8320, 0.5817, 0.4281,  ..., 0.4774, 0.7369, 0.3399],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6512, 0.6070, 0.9859,  ..., 0.1132, 0.3424, 0.8340],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6480, 0.8109, 0.4544,  ..., 0.3748, 0.6372, 0.5589],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.5503, 0.6517, 0.9337,  ..., 0.6286, 0.3071, 0.8045],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6374, 0.7521, 0.4551,  ..., 0.2520, 0.3896, 0.5138],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8282, 0.8041, 0.9017,  ..., 0.6386, 0.7634, 0.6606],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8210, 0.6500, 0.6908,  ..., 0.7337, 0.3871, 0.7528],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7841, 0.7948, 0.8970,  ..., 0.6667, 0.8244, 0.5615],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6782, 0.8208, 0.9177,  ..., 0.5855, 0.5658, 0.8165],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9743, 0.8561, 0.6769,  ..., 0.6992, 0.6368, 0.8248],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4474, 0.7188, 0.8170,  ..., 0.4867, 0.5188, 0.7117],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.4218, 0.3429, 0.7012,  ..., 0.4153, 0.6929, 0.4209],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7826, 0.7826, 0.3284,  ..., 0.5918, 0.6767, 0.2721],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7953, 0.7882, 0.3938,  ..., 0.6651, 0.4376, 0.3724],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7390, 0.4420, 0.7948,  ..., 0.1694, 0.5399, 0.3960],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7501, 0.8178, 0.8090,  ..., 0.5578, 0.5871, 0.5755],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6535, 0.9496, 0.5352,  ..., 0.5747, 0.6406, 0.6317],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.8097, 0.8090, 0.7812,  ..., 0.4182, 0.6145, 0.5877],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.7166, 0.7711, 0.8210,  ..., 0.4506, 0.4357, 0.6399],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.3902, 0.9941, 0.5804,  ..., 0.4859, 0.1860, 0.5981],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.6762, 0.8936, 0.6942,  ..., 0.6436, 0.4852, 0.6061],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.9247, 0.8343, 0.6029,  ..., 0.6099, 0.8565, 0.8262],\n",
              "        grad_fn=<ToCopyBackward0>)]"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data_test[\"track\", \"contains\", \"playlist\"].edge_label_index.t()[model(data_test.to(device)) > 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "out = model(data_test.to(device))\n",
        "len(out > 0.5)/len(out)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "fdd1bf0cdef9f431ef204eb65428406a6292ed13583a3a7833f3ab005ff2b93a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
