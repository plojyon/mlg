{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will train a GNN to perform link prediction on a heterogenous graph from the Spotify Million Playlists dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/home/yon/jupyter-server/mlg/src/')\n",
    "\n",
    "import loader\n",
    "import config\n",
    "import model as M\n",
    "import preprocessing\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import BinaryAccuracy\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch_geometric.nn.SAGEConv((-1, -1), hidden_channels, normalize=True)\n",
    "        self.conv2 = torch_geometric.nn.SAGEConv((-1, -1), hidden_channels, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def forward(self, x_track, x_playlist, track_playlist_edge):\n",
    "        track_embedding = x_track[track_playlist_edge[0]]\n",
    "        playlist_embedding = x_playlist[track_playlist_edge[1]]\n",
    "\n",
    "        #print(playlist_embedding)\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (playlist_embedding * track_embedding).sum(dim=-1)\n",
    "\n",
    "class HeteroModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, node_features, metadata):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for users and movies:\n",
    "\n",
    "        self.node_lin = {\n",
    "            k: torch.nn.Linear(v.shape[1], hidden_channels) for k, v in node_features.items()\n",
    "        }\n",
    "\n",
    "        for _, v in self.node_lin.items():\n",
    "            torch.nn.init.xavier_uniform_(v.weight)\n",
    "        \n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = torch_geometric.nn.to_hetero(self.gnn, metadata=metadata)\n",
    "\n",
    "        self.classifier = LinkPredictor()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict = {\n",
    "            k: self.node_lin[k](v) for k, v in data.x_dict.items()\n",
    "        }\n",
    "\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"track\"],\n",
    "            x_dict[\"playlist\"],\n",
    "            data[\"track\", \"contains\", \"playlist\"].edge_label_index,\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for _, v in self.node_lin.items():\n",
    "            torch.nn.init.xavier_uniform_(v.weight)\n",
    "        self.gnn.reset_parameters()\n",
    "\n",
    "def dummy_generator(source):\n",
    "    for e in source:\n",
    "        yield e\n",
    "\n",
    "def train(model, train_loader, optimizer, batch_wrapper=dummy_generator):\n",
    "    model.train()\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for i, batch in enumerate(batch_wrapper(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch)\n",
    "        truth = batch[\"track\", \"contains\", \"playlist\"].edge_label\n",
    "\n",
    "\n",
    "        ind = torch.randint(len(out),(5,))\n",
    "\n",
    "        print(out[ind])\n",
    "        print(truth[ind])\n",
    "\n",
    "        if(i % 10 == 0):\n",
    "            #print(out[:10])\n",
    "            #print(batch[\"track\", \"contains\", \"playlist\"].edge_label[:10])\n",
    "            pass\n",
    "        loss = torch.nn.functional.mse_loss(\n",
    "            out, truth\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric = BinaryAccuracy()\n",
    "        metric.update(out, truth)\n",
    "        accuracy += metric.compute() * len(out)\n",
    "\n",
    "        total_examples += len(out)\n",
    "        total_loss += float(loss) * len(out)\n",
    "\n",
    "    return total_loss / total_examples, accuracy / total_examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm spotify_million_playlist_dataset/pickles/G_example.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghetero = loader.get_ghetero(True, config)\n",
    "data_train, data_val, data_test = loader.get_datasets(True, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training mask for playlist nodes\n",
    "train_mask = torch.zeros(ghetero[\"playlist\"].x.shape[0], dtype=torch.bool)\n",
    "train_mask[torch.randperm(train_mask.shape[0])[:int(train_mask.shape[0]*0.8)]] = True\n",
    "\n",
    "ghetero[\"playlist\"].train_mask = train_mask\n",
    "\n",
    "ghetero[\"playlist\"].y = torch.LongTensor([1]*ghetero[\"playlist\"].x.shape[0])\n",
    "\n",
    "model = HeteroModel(64, ghetero.x_dict, ghetero.metadata())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.0001)\n",
    "\n",
    "edge_label_index = data_train[\"track\", \"contains\", \"playlist\"].edge_label_index\n",
    "edge_label = data_train[\"track\", \"contains\", \"playlist\"].edge_label\n",
    "train_loader = torch_geometric.loader.LinkNeighborLoader(\n",
    "    data=data_train,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio=2.0,\n",
    "    edge_label_index=((\"track\", \"contains\", \"playlist\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████▉                              | 72/125 [00:02<00:02, 24.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3750, 0.3662, 0.3816, 0.1446, 0.5960], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.1992, 0.1685, 0.5131, 0.5703, 0.3960], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3947, 0.5004, 0.2134, 0.4695, 0.6114], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3920, 0.2098, 0.3707, 0.3833, 0.4864], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4408, 0.2041, 0.4373, 0.4451, 0.2018], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████▎                          | 78/125 [00:03<00:01, 24.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5524, 0.5033, 0.4474, 0.2355, 0.4444], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.1784, 0.1774, 0.4460, 0.3866, 0.4394], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2579, 0.4547, 0.3015, 0.2520, 0.2824], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2311, 0.4808, 0.4790, 0.4165, 0.3404], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3964, 0.2738, 0.2673, 0.3435, 0.3942], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████                         | 81/125 [00:03<00:01, 24.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1732, 0.5817, 0.2712, 0.3862, 0.2859], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4130, 0.2108, 0.4208, 0.3669, 0.1776], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.1650, 0.5099, 0.3373, 0.4894, 0.4570], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3814, 0.3621, 0.3758, 0.4898, 0.4129], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3560, 0.4385, 0.3821, 0.4499, 0.4640], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3606, 0.4053, 0.3558, 0.3519, 0.3375], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████▍                     | 87/125 [00:03<00:01, 24.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4584, 0.5027, 0.6118, 0.5276, 0.3860], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2224, 0.0761, 0.1981, 0.1135, 0.0788], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4438, 0.4096, 0.4067, 0.4780, 0.3979], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.7312, 0.6019, 0.4199, 0.4588, 0.4929], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2446, 0.3687, 0.3432, 0.3636, 0.3654], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.1458, 0.2294, 0.2963, 0.1490, 0.2900], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████▊                  | 93/125 [00:03<00:01, 24.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3251, 0.2771, 0.3420, 0.3440, 0.6328], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.6583, 0.2956, 0.6215, 0.3288, 0.2958], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2646, 0.1358, 0.2642, 0.1763, 0.2422], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.7657, 1.0528, 0.6223, 0.9340, 0.6164], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([-0.0355,  0.0773, -0.0871,  0.1083,  0.1371], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████▏              | 99/125 [00:03<00:01, 25.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0180, -0.0060,  0.1545,  0.0198,  0.1437], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.5932, 0.6368, 0.5361, 0.5805, 0.6522], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.5262, 0.5574, 0.4622, 0.4858, 0.5668], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([-0.0421, -0.0056, -0.1265,  0.1175, -0.1069], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.5869, 0.5493, 0.5895, 0.5699, 0.5376], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4456, 0.4420, 0.5382, 0.5066, 0.4133], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████▊           | 105/125 [00:04<00:00, 25.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2197, 0.2057, 0.2042, 0.2652, 0.3114], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.1545, 0.1973, 0.1107, 0.0340, 0.0569], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2863, 0.2960, 0.2741, 0.3105, 0.2827], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3822, 0.4301, 0.4729, 0.4558, 0.4761], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.5080, 0.5149, 0.4907, 0.4590, 0.5403], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████▏       | 111/125 [00:04<00:00, 24.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4057, 0.4367, 0.3263, 0.4163, 0.4169], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.1827, 0.2691, 0.2705, 0.2750, 0.2443], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2435, 0.2746, 0.2430, 0.2418, 0.2253], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3571, 0.3409, 0.3243, 0.2961, 0.3359], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4331, 0.4411, 0.4329, 0.4142, 0.3713], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████▊      | 114/125 [00:04<00:00, 24.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3634, 0.4707, 0.4821, 0.4461, 0.4501], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4180, 0.4092, 0.4485, 0.4018, 0.3020], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3214, 0.3231, 0.3352, 0.3106, 0.3250], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.2310, 0.3599, 0.3504, 0.3725, 0.2409], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4151, 0.3847, 0.4031, 0.4906, 0.3867], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.4050, 0.3826, 0.4093, 0.4698, 0.2501], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████    | 118/125 [00:04<00:00, 24.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2166, 0.3393, 0.2892, 0.3508, 0.3315], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.3758, 0.2162, 0.2274, 0.3131, 0.3785], grad_fn=<SliceBackward0>)\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m accuracies \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m---> 11\u001b[0m     loss, accuracy \u001b[39m=\u001b[39m train(model, train_loader, optimizer, batch_wrapper\u001b[39m=\u001b[39;49mtqdm\u001b[39m.\u001b[39;49mtqdm)\n\u001b[1;32m     12\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     13\u001b[0m     accuracies\u001b[39m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[121], line 91\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, batch_wrapper)\u001b[0m\n\u001b[1;32m     88\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     89\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 91\u001b[0m metric \u001b[39m=\u001b[39m BinaryAccuracy()\n\u001b[1;32m     92\u001b[0m metric\u001b[39m.\u001b[39mupdate(out, truth)\n\u001b[1;32m     93\u001b[0m accuracy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(out)\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib/python3.10/site-packages/torcheval/metrics/classification/accuracy.py:184\u001b[0m, in \u001b[0;36mBinaryAccuracy.__init__\u001b[0;34m(self, threshold, device)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m: TBinaryAccuracy,\n\u001b[1;32m    180\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m    181\u001b[0m     threshold: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m,\n\u001b[1;32m    182\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib/python3.10/site-packages/torcheval/metrics/classification/accuracy.py:98\u001b[0m, in \u001b[0;36mMulticlassAccuracy.__init__\u001b[0;34m(self, average, num_classes, k, device)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_state(\u001b[39m\"\u001b[39m\u001b[39mnum_correct\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_state(\u001b[39m\"\u001b[39;49m\u001b[39mnum_total\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m0.0\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39m# num_classes is verified to be not None when average != \"micro\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_state(\n\u001b[1;32m    102\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnum_correct\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    103\u001b[0m         torch\u001b[39m.\u001b[39mzeros(num_classes \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m    104\u001b[0m     )\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib/python3.10/site-packages/torcheval/metrics/metric.py:67\u001b[0m, in \u001b[0;36mMetric._add_state\u001b[0;34m(self, name, default)\u001b[0m\n\u001b[1;32m     64\u001b[0m _check_state_variable_type(name, default)\n\u001b[1;32m     65\u001b[0m \u001b[39m# deepcopy makes sure the input/initial value/default value of the state\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# variable are independent.\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, deepcopy(default))\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state_name_to_default[name] \u001b[39m=\u001b[39m deepcopy(default)\n",
      "File \u001b[0;32m/usr/lib64/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib64/python3.10/site-packages/torch/_tensor.py:119\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)]\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    109\u001b[0m     \u001b[39m# TODO: skipping storage copy is wrong for meta, as meta\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39m# does accurate alias tracking; however, the code below\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[39m# doesn't work because of\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/47442\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39m# Update the test in test_serialization if you remove 'meta' from here\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    115\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_sparse\n\u001b[1;32m    116\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype\n\u001b[1;32m    117\u001b[0m         \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mlazy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mort\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhpu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mipu\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    118\u001b[0m         \u001b[39mor\u001b[39;00m (\n\u001b[0;32m--> 119\u001b[0m             \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_has_storage(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    120\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprivateuseone\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_ptr() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[1;32m    124\u001b[0m         new_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    125\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(new_tensor) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "epoch = 100\n",
    "\n",
    "render_graph = True\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    loss, accuracy = train(model, train_loader, optimizer, batch_wrapper=tqdm.tqdm)\n",
    "    losses.append(loss)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Epoch {i+1}/{epoch}, Loss: {loss:.4f}, Accuracy {accuracy:.4f}\")\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(np.arange(len(accuracies)), accuracies)\n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "\n",
    "    #start plot at 0\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    if render_graph and i % 10 == 0 and i != 0:\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdd1bf0cdef9f431ef204eb65428406a6292ed13583a3a7833f3ab005ff2b93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
