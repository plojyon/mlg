{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/home/yon/jupyter-server/mlg/src/')\n",
    "\n",
    "import config\n",
    "import loader\n",
    "import model as m\n",
    "import preprocessing\n",
    "\n",
    "# if u want to use the big config\n",
    "#config = config.big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary imports (delete later)\n",
    "import sys  \n",
    "sys.path.insert(0, '/home/yon/jupyter-server/mlg/src/')\n",
    "\n",
    "import loader\n",
    "import config\n",
    "import model as M\n",
    "import preprocessing\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch_geometric.transforms as T\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carloss62.pkl\t      ghetero_example.pkl     top-ghetero-1000.pkl\n",
      "carloss66.pkl\t      ghetero.pkl\t      top-ghetero-5000-fixed-maybe.pkl\n",
      "carloss72.pkl\t      G.pkl\t\t      top-ghetero-5000.pkl\n",
      "datasets_example.pkl  index.pkl\t\t      top-idx-1000.pkl\n",
      "datasets.pkl\t      top-G-1000_example.pkl  top-idx-5000.pkl\n",
      "deleteme.pkl\t      top-G-1000.pkl\n",
      "G_example.pkl\t      top-G-5000.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls spotify_million_playlist_dataset/pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = pickle.load(open(config.pickles + \"/top-idx-1000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['playlist', 'track', 'artist', 'album'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:32<00:00,  2.55files/s]\n"
     ]
    }
   ],
   "source": [
    "index = {\n",
    "    \"playlist\": {},\n",
    "    \"track\": {},\n",
    "    \"artist\": {},\n",
    "    \"album\": {},\n",
    "}\n",
    "\n",
    "dataset_path = config.big.dataset_path\n",
    "filenames = os.listdir(dataset_path)\n",
    "for i in tqdm(range(len(filenames)), unit=\"files\"):\n",
    "    with open(os.path.join(dataset_path, filenames[i])) as json_file:\n",
    "        playlists = json.load(json_file)[\"playlists\"]\n",
    "        for playlist in playlists:\n",
    "            playlist_name = f\"spotify:playlist:{playlist['pid']}\"\n",
    "            index[\"playlist\"][playlist_name] = playlist[\"name\"]\n",
    "            for track in playlist[\"tracks\"]:\n",
    "                index[\"track\"][track[\"track_uri\"]] = track[\"track_name\"]\n",
    "                index[\"artist\"][track[\"artist_uri\"]] = track[\"artist_name\"]\n",
    "                index[\"album\"][track[\"album_uri\"]] = track[\"album_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pickle.load(open(config.pickles + \"/index.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['playlist', 'track', 'artist', 'album'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spotify:playlist:549000',\n",
       " 'spotify:playlist:549001',\n",
       " 'spotify:playlist:549002',\n",
       " 'spotify:playlist:549003',\n",
       " 'spotify:playlist:549004']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index[\"playlist\"].keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = {key: {} for key in index.keys()}\n",
    "for typ in index.keys():\n",
    "    for key in index[typ].keys():\n",
    "        key2 = key.split(\":\")[2]\n",
    "        index2[typ][key2] = index[typ][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(index2, open(config.pickles + \"/index.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(index, open(config.pickles + \"/index.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000] started\n",
      "[5000] got top n playlists in 1.4853570461273193 seconds\n",
      "[5000] finshed getting neighbors in 5315.407293796539 seconds (288032 nodes))\n",
      "[5000] finished subgraphing in 43.11258053779602 seconds\n",
      "[5000] finished pickling in 0.982130765914917 seconds\n"
     ]
    }
   ],
   "source": [
    "G = pickle.load(open(config.pickles + \"/G.pkl\", \"rb\"))\n",
    "preprocessing.smart_split(G, splits=[5000], pickle_location=config.pickled_top_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved node index to spotify_million_playlist_dataset/pickles/top-idx-5000.pkl\n"
     ]
    }
   ],
   "source": [
    "# G = loader.get_g()\n",
    "# preprocessing.smart_split(G, splits=[100], pickle_location=lambda _:config.pickles + \"/deleteme.pkl\")\n",
    "topG = pickle.load(open(config.pickles + \"/top-G-5000_example.pkl\", \"rb\"))\n",
    "ghetero = loader.nx2hetero(topG, config.pickles + \"/top-idx-5000.pkl\")\n",
    "pickle.dump(ghetero, open(config.pickles + \"/top-ghetero-5000.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ghetero from pickle ...\n",
      "Loading datasets from pickle ...\n"
     ]
    }
   ],
   "source": [
    "ghetero = loader.get_data()\n",
    "data_train, data_val, data_test = loader.get_datasets()\n",
    "ghetero = ghetero.to(device)\n",
    "data_train = data_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkNeighborLoader(torch_geometric.loader.LinkLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        num_neighbors,\n",
    "        edge_label_index = None,\n",
    "        edge_label = None,\n",
    "        edge_label_time = None,\n",
    "        replace = False,\n",
    "        directed = True,\n",
    "        disjoint = False,\n",
    "        temporal_strategy = 'uniform',\n",
    "        neg_sampling = None,\n",
    "        neg_sampling_ratio = None,\n",
    "        time_attr = None,\n",
    "        transform = None,\n",
    "        is_sorted = False,\n",
    "        filter_per_worker = False,\n",
    "        neighbor_sampler = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # TODO(manan): Avoid duplicated computation (here and in NodeLoader):\n",
    "        edge_type, _ = torch_geometric.loader.utils.get_edge_label_index(data, edge_label_index)\n",
    "\n",
    "        if (edge_label_time is not None) != (time_attr is not None):\n",
    "            raise ValueError(\"I fart\")\n",
    "\n",
    "        if neighbor_sampler is None:\n",
    "            neighbor_sampler = torch_geometric.sampler.NeighborSampler(\n",
    "                data,\n",
    "                num_neighbors=num_neighbors,\n",
    "                replace=replace,\n",
    "                directed=directed,\n",
    "                disjoint=disjoint,\n",
    "                temporal_strategy=temporal_strategy,\n",
    "                input_type=edge_type,\n",
    "                time_attr=time_attr,\n",
    "                is_sorted=is_sorted,\n",
    "                share_memory=kwargs.get('num_workers', 0) > 0,\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            data=data,\n",
    "            link_sampler=neighbor_sampler,\n",
    "            edge_label_index=edge_label_index,\n",
    "            edge_label=edge_label,\n",
    "            edge_label_time=edge_label_time,\n",
    "            neg_sampling=neg_sampling,\n",
    "            neg_sampling_ratio=neg_sampling_ratio,\n",
    "            transform=transform,\n",
    "            filter_per_worker=filter_per_worker,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'transform_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m ghetero[\u001b[39m\"\u001b[39m\u001b[39mplaylist\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtrain_mask \u001b[39m=\u001b[39m train_mask\n\u001b[1;32m     60\u001b[0m ghetero[\u001b[39m\"\u001b[39m\u001b[39mplaylist\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mghetero[\u001b[39m\"\u001b[39m\u001b[39mplaylist\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 62\u001b[0m model \u001b[39m=\u001b[39m Impostor(data_train)\n\u001b[1;32m     63\u001b[0m \u001b[39m# model = model.to(device)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mImpostor.__init__\u001b[0;34m(self, data_train)\u001b[0m\n\u001b[1;32m     16\u001b[0m g \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator(device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m g\u001b[39m.\u001b[39mmanual_seed(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader \u001b[39m=\u001b[39m LinkNeighborLoader(\n\u001b[1;32m     19\u001b[0m     data\u001b[39m=\u001b[39;49mdata_train,\n\u001b[1;32m     20\u001b[0m     num_neighbors\u001b[39m=\u001b[39;49m[\u001b[39m20\u001b[39;49m, \u001b[39m10\u001b[39;49m],\n\u001b[1;32m     21\u001b[0m     neg_sampling_ratio\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m     edge_label_index\u001b[39m=\u001b[39;49m((\u001b[39m\"\u001b[39;49m\u001b[39mtrack\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontains\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mplaylist\u001b[39;49m\u001b[39m\"\u001b[39;49m), edge_label_index),\n\u001b[1;32m     23\u001b[0m     edge_label\u001b[39m=\u001b[39;49medge_label,\n\u001b[1;32m     24\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     26\u001b[0m     generator\u001b[39m=\u001b[39;49mg,\n\u001b[1;32m     27\u001b[0m     transform_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     28\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mLinkNeighborLoader.__init__\u001b[0;34m(self, data, num_neighbors, edge_label_index, edge_label, edge_label_time, replace, directed, disjoint, temporal_strategy, neg_sampling, neg_sampling_ratio, time_attr, transform, is_sorted, filter_per_worker, neighbor_sampler, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m neighbor_sampler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     neighbor_sampler \u001b[39m=\u001b[39m torch_geometric\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mNeighborSampler(\n\u001b[1;32m     30\u001b[0m         data,\n\u001b[1;32m     31\u001b[0m         num_neighbors\u001b[39m=\u001b[39mnum_neighbors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m         share_memory\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 42\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     43\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m     44\u001b[0m     link_sampler\u001b[39m=\u001b[39;49mneighbor_sampler,\n\u001b[1;32m     45\u001b[0m     edge_label_index\u001b[39m=\u001b[39;49medge_label_index,\n\u001b[1;32m     46\u001b[0m     edge_label\u001b[39m=\u001b[39;49medge_label,\n\u001b[1;32m     47\u001b[0m     edge_label_time\u001b[39m=\u001b[39;49medge_label_time,\n\u001b[1;32m     48\u001b[0m     neg_sampling\u001b[39m=\u001b[39;49mneg_sampling,\n\u001b[1;32m     49\u001b[0m     neg_sampling_ratio\u001b[39m=\u001b[39;49mneg_sampling_ratio,\n\u001b[1;32m     50\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m     51\u001b[0m     filter_per_worker\u001b[39m=\u001b[39;49mfilter_per_worker,\n\u001b[1;32m     52\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter-server/venv/lib/python3.10/site-packages/torch_geometric/loader/link_loader.py:165\u001b[0m, in \u001b[0;36mLinkLoader.__init__\u001b[0;34m(self, data, link_sampler, edge_label_index, edge_label, edge_label_time, neg_sampling, neg_sampling_ratio, transform, filter_per_worker, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_data \u001b[39m=\u001b[39m InputData(\n\u001b[1;32m    158\u001b[0m     edge_label_index[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mclone(),\n\u001b[1;32m    159\u001b[0m     edge_label_index[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mclone(),\n\u001b[1;32m    160\u001b[0m     edge_label,\n\u001b[1;32m    161\u001b[0m     edge_label_time,\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(edge_label_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 165\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(iterator, collate_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataLoader.__init__() got an unexpected keyword argument 'transform_fn'"
     ]
    }
   ],
   "source": [
    "class Impostor(torch.nn.Module):\n",
    "    \"\"\"Intelligent Mmusic Prediction Operating System with Top-level Optimization and Recommendation.\"\"\"\n",
    "\n",
    "    def __init__(self, data_train):\n",
    "        super().__init__()\n",
    "        self.model = m.HeteroModel(64, ghetero.x_dict, ghetero.metadata())\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "\n",
    "        edge_label_index = data_train[\"track\", \"contains\", \"playlist\"].edge_label_index\n",
    "\n",
    "        edge_label = data_train[\"track\", \"contains\", \"playlist\"].edge_label\n",
    "\n",
    "        edge_label_index = edge_label_index.to(device)\n",
    "        data_train = data_train.to(device)\n",
    "\n",
    "        g = torch.Generator(device=\"cuda\")\n",
    "        g.manual_seed(0)\n",
    "        self.train_loader = LinkNeighborLoader(\n",
    "            data=data_train,\n",
    "            num_neighbors=[20, 10],\n",
    "            neg_sampling_ratio=2.0,\n",
    "            edge_label_index=((\"track\", \"contains\", \"playlist\"), edge_label_index),\n",
    "            edge_label=edge_label,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            generator=g,\n",
    "            transform_fn=lambda x: x.to(device),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model.forward(data)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        total_examples = total_loss = 0\n",
    "        for batch in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_size = 100\n",
    "            out = self.model(batch)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                out, batch[\"track\", \"contains\", \"playlist\"].edge_label\n",
    "            )\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_examples += batch_size\n",
    "            print(f'Loss: {loss:.4f}')\n",
    "            total_loss += float(loss) * batch_size\n",
    "\n",
    "        return total_loss / total_examples\n",
    "\n",
    "# create training mask for playlist nodes\n",
    "train_mask = torch.zeros(ghetero[\"playlist\"].x.shape[0], dtype=torch.bool)\n",
    "train_mask[torch.randperm(train_mask.shape[0])[:int(train_mask.shape[0]*0.8)]] = True\n",
    "train_mask = train_mask.to(device)\n",
    "\n",
    "ghetero[\"playlist\"].train_mask = train_mask\n",
    "\n",
    "ghetero[\"playlist\"].y = torch.LongTensor([1]*ghetero[\"playlist\"].x.shape[0])\n",
    "\n",
    "model = Impostor(data_train)\n",
    "# model = model.to(device)\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
